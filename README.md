# MEDIATECH

[![License](https://img.shields.io/github/license/etalab-ia/mediatech?label=licence&color=red)](https://github.com/etalab-ia/mediatech/blob/main/LICENSE)
[![Hugging Face collection](https://img.shields.io/badge/ğŸ¤—-Hugging%20Face%20collection-yellow)](https://huggingface.co/collections/hulk10/mediatech)


## ğŸ“ Description

MEDIATECH automatise la collecte, l'extraction et la publication de jeux de donnÃ©es publics franÃ§ais sur Hugging Face. Ce projet facilite l'accÃ¨s aux donnÃ©es administratives franÃ§aises pour les applications d'IA en maintenant Ã  jour la collection [hulk10/mediatech](https://huggingface.co/collections/hulk10/mediatech).

## ğŸ¯ Objectif

Mettre Ã  jour automatiquement les datasets de la collection [hulk10/mediatech](https://huggingface.co/collections/hulk10/mediatech) Ã  partir des sources officielles franÃ§aises (DILA, data.gouv.fr, etc.).

## ï¿½ Installation

### 1. Installer les dÃ©pendances

```bash
make install
```

Ou manuellement avec `uv` :

```bash
uv sync --all-groups --dev
```

### 2. Configuration

Assure-toi d'avoir configurÃ© ton token Hugging Face pour l'upload :

```bash
huggingface-cli login
```

## ğŸ“– Usage

### Pipeline complet

Le pipeline se compose de deux Ã©tapes principales :

#### 1. TÃ©lÃ©chargement des donnÃ©es

```bash
PYTHONPATH=./ uv run scripts/download.py --download_name <dataset> [options]
```

**Options disponibles :**
- `--download_name` : Nom du dataset Ã  tÃ©lÃ©charger (voir [config/data_config.json](config/data_config.json))
- `--start_date` : Date de dÃ©but au format YYYY-MM-DD (dÃ©faut: 2025-10-16)
- `--end_date` : Date de fin au format YYYY-MM-DD (dÃ©faut: aujourd'hui)
- `--max_download` : Nombre maximum de fichiers (-1 pour illimitÃ©)

**Exemples :**

```bash
# TÃ©lÃ©charger LEGI depuis 2021
PYTHONPATH=./ uv run scripts/download.py --download_name legi --start_date 2021-01-01

# TÃ©lÃ©charger CNIL pour la derniÃ¨re semaine
PYTHONPATH=./ uv run scripts/download.py --download_name cnil --start_date 2025-12-25

# TÃ©lÃ©charger DOLE
PYTHONPATH=./ uv run scripts/download.py --download_name dole
```

#### 2. Extraction et export vers Hugging Face

```bash
PYTHONPATH=./ uv run scripts/extraction_and_export.py --download_name <dataset> --user-id hulk10 [options]
```

**Options disponibles :**
- `--download_name` : Nom du dataset Ã  extraire
- `--user-id` : ID utilisateur Hugging Face (hulk10)
- `--start_date` : Date de dÃ©but (dÃ©faut: 2025-10-16)
- `--end_date` : Date de fin (dÃ©faut: aujourd'hui)

**Exemples :**

```bash
# Extraire et publier LEGI
PYTHONPATH=./ uv run scripts/extraction_and_export.py --download_name legi --user-id hulk10

# Extraire CNIL pour une pÃ©riode spÃ©cifique
PYTHONPATH=./ uv run scripts/extraction_and_export.py \
  --download_name cnil \
  --user-id hulk10 \
  --start_date 2025-01-01 \
  --end_date 2025-12-31
```

### Datasets disponibles

Les datasets configurÃ©s dans [config/data_config.json](config/data_config.json) incluent :

- **legi** : LÃ©gislation franÃ§aise (DILA)
- **cnil** : DÃ©cisions de la CNIL
- **constit** : DÃ©cisions du Conseil Constitutionnel
- **dole** : DÃ©cisions du Journal Officiel
- **service_public_pro** : Fiches Service-Public.fr Pro
- **service_public_part** : Fiches Service-Public.fr Particuliers
- **state_administrations_directory** : Annuaire des administrations d'Ã‰tat
- **local_administrations_directory** : Annuaire des administrations locales
- **data_gouv_datasets_catalog** : Catalogue des datasets data.gouv.fr
- **travail_emploi** : Fiches Travail-Emploi

## ğŸ”„ Workflow type

```bash
# 1. TÃ©lÃ©charger les donnÃ©es
PYTHONPATH=./ uv run scripts/download.py --download_name legi --start_date 2024-01-01

# 2. Extraire et publier sur Hugging Face
PYTHONPATH=./ uv run scripts/extraction_and_export.py --download_name legi --user-id hulk10
```

Les donnÃ©es sont automatiquement :
1. TÃ©lÃ©chargÃ©es depuis les sources officielles
2. Extraites et traitÃ©es
3. PartitionnÃ©es en Parquet
4. PubliÃ©es sur [https://huggingface.co/hulk10](https://huggingface.co/hulk10)

## ï¿½ Structure du projet

```
mediatech/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download.py                    # TÃ©lÃ©chargement des archives
â”‚   â””â”€â”€ extraction_and_export.py       # Extraction et export vers HF
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ download/                      # Modules de tÃ©lÃ©chargement
â”‚   â”œâ”€â”€ extraction/                    # Modules d'extraction
â”‚   â”œâ”€â”€ exports/                       # Modules d'export
â”‚   â”œâ”€â”€ process/                       # Modules de traitement
â”‚   â””â”€â”€ utils/                         # Utilitaires
â”œâ”€â”€ config/
â”‚   â””â”€â”€ data_config.json              # Configuration des sources
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ unprocessed/                  # DonnÃ©es brutes tÃ©lÃ©chargÃ©es
â”‚   â”œâ”€â”€ extracted/                    # DonnÃ©es extraites
â”‚   â””â”€â”€ {dataset}/data/               # Parquets partitionnÃ©s
â”œâ”€â”€ Makefile                          # Commandes utilitaires
â””â”€â”€ pyproject.toml                    # Configuration Python

## âš–ï¸ License

ThiğŸ› ï¸ DÃ©veloppement

### Commandes utiles

```bash
# Installer les dÃ©pendances
make install

# Lancer les tests
make run-test

# Linter le code
make lint

# Nettoyer les caches
make clean
```

### Ajouter un nouveau dataset

1. Ajoute la configuration dans [config/data_config.json](config/data_config.json)
2. CrÃ©e les modules de tÃ©lÃ©chargement, extraction et export nÃ©cessaires dans `src/`
3. Teste avec les scripts

## ğŸ¤— Collection Hugging Face

Les datasets sont publiÃ©s dans la collection :  
**[https://huggingface.co/collections/hulk10/mediatech](https://huggingface.co/collections/hulk10/mediatech)**

Chaque dataset est disponible au format Parquet partitionnÃ© par fichier source, facilitant l'accÃ¨s incrÃ©mental et la mise Ã  jour.

## âš–ï¸ Licence

Ce projet est sous licence [MIT License](./LICENSE).

---

**Maintenu par** : [hulk10](https://huggingface.co/hulk10)  
**Collection** : [hulk10/mediatech](https://huggingface.co/collections/hulk10/mediatech)